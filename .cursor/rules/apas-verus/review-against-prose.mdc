# Review Chapter Against Prose

When the user says "review ChapNN" or "review against prose":

## Phase 1: Inventory (tool-generated)

Run veracity to generate the function inventory and spec strengths:

```bash
~/projects/veracity/target/release/veracity-review-module-fn-impls -d src/ChapNN
```

Then classify spec strengths per the `classify-spec-strengths` rule. This gives us:
- Every function in the chapter
- Whether it's in a trait, impl, or module-level
- Whether it's inside `verus!`
- Spec strength (strong / partial / weak / none)
- Proof holes

This is the mechanical baseline. Don't duplicate what the tool already produces.

## Phase 2: Prose Inventory (manual)

Read `prompts/ChapNN.txt`. Extract every named item into categories:

- **Definitions**: Named ADTs, type classes, abstract interfaces
- **Algorithms**: Pseudocode with a name (e.g., "Algorithm 21.1", "insertion sort")
- **Cost specs**: Every stated Work and Span bound
- **Theorems/Properties**: Correctness claims, invariants, bounds
- **Exercises/Problems**: Numbered items that the code may implement

## Phase 3: Algorithmic Analysis (the review)

For each executable function (`fn`, not `spec fn` or `proof fn`):

### 3a. Cost annotations in source

Write two doc comment lines directly before the function:

```rust
/// - APAS: Work Θ(...), Span Θ(...)
/// - Claude-Opus-4.6: Work Θ(...), Span Θ(...) — [reason if different]
```

Rules for the two lines:

**APAS line**: What the textbook says the cost *should* be for this algorithm.
If the prose doesn't state a cost for this specific function, write:
```rust
/// - APAS: (no cost stated)
```
If the function has no prose counterpart at all, write:
```rust
/// - APAS: N/A — Verus-specific scaffolding.
```

**Claude-Opus-4.6 line**: What the code *actually* achieves, based on reading the
implementation. Three outcomes:

1. **Agree**: `/// - Claude-Opus-4.6: Work Θ(...), Span Θ(...) — agrees with APAS.`
2. **Disagree**: `/// - Claude-Opus-4.6: Work Θ(...), Span Θ(...) — [specific reason for difference]`
3. **Cannot determine**: `/// - Claude-Opus-4.6: Cost not analyzable — [reason, e.g., external_body]`

The disagreement reason must be concrete. Not "differs from APAS" but
"closures call fib_seq not fib_par, so only top-level split is parallel"
or "uses linear scan where APAS assumes O(1) hash lookup."

### 3b. Implementation fidelity

For each function that implements a prose algorithm, note whether the code
follows the prose algorithm or deviates. Deviations are not necessarily wrong —
but they must be noted because they can change the cost.

Common deviations:
- Sequential where APAS says parallel (or vice versa)
- Different data structure than APAS assumes (e.g., Vec where APAS says array with O(1) slice)
- Missing recursive parallelism (like fib_par calling fib_seq)
- Granularity cutoffs not in the prose (acceptable, note them)

### 3c. Spec fidelity

For each function with requires/ensures, compare against the prose:
- Does `requires` capture the prose's stated preconditions?
- Does `ensures` capture the prose's stated postconditions?
- Are there prose properties that the spec doesn't express?

This complements the veracity spec-strength classification — spec strength
tells you whether a spec exists and how complete it looks structurally;
spec fidelity tells you whether it matches what the textbook actually claims.

## Phase 4: Parallelism Review

For every `*Mt*` (multi-threaded) module, check whether each operation is
genuinely parallel or just thread-safe.

### 4a. Classify each Mt function

For each exec function in a `*Mt*` module, determine:

1. **Parallel** — spawns threads, uses `join`, or calls `HFSchedulerMtEph`
   spawn/wait. The Span reflects actual parallelism.
2. **Sequential** — uses a sequential loop (for/while/loop) with no spawning.
   Thread-safe (Send + Sync bounds) but Span == Work. The APAS Span annotation
   may be aspirational rather than achieved.
3. **Delegating** — calls the St (single-threaded) variant or another sequential
   helper. Same as sequential.

### 4b. Span audit

For each Mt function annotated with `Span Θ(...)`:
- If the function is parallel, verify the Span matches the parallelism structure
  (e.g., spawn-per-element gives Span Θ(element-cost), fork-join gives Span
  Θ(log n × element-cost)).
- If the function is sequential, the Span equals the Work. Flag any annotation
  where Span < Work — that Span is aspirational, not achieved.
- Note: an aspirational Span is not wrong in the APAS line (it's what the
  textbook *intends*), but the Claude-Opus-4.6 line must state the actual Span.

### 4c. Parallelism gap table

Produce a table:

| Function | APAS Span | Actual | Parallel? | Notes |
|----------|-----------|--------|-----------|-------|

This makes it immediately visible which Mt operations still need parallel
implementations.

## Phase 5: Runtime Test Review

Check that every chapter module has a corresponding runtime test file in `tests/ChapNN/`.

For each source module `src/ChapNN/FooStBar.rs`, expect `tests/ChapNN/TestFooStBar.rs`.

### 5a. Coverage check

- List all exec functions in the module (from Phase 1 inventory).
- List all test functions in the test file.
- Flag exec functions with no test coverage.
- Flag test functions that test deleted or renamed functions.

### 5b. Test quality

For each test, assess:
- Does it exercise the happy path (valid inputs, expected outputs)?
- Does it exercise edge cases (empty inputs, singleton, boundary values)?
- Does it test the spec-relevant properties (the things `ensures` promises)?
- For types with `PartialEq`, does it test equality?

### 5c. Missing tests

Propose new tests for uncovered exec functions. Prioritize:
1. Functions with strong specs — a runtime test validates the spec informally.
2. Functions with proof holes (`external_body`, `assume`) — a runtime test is the
   only evidence the implementation is correct.
3. Functions used as building blocks by other chapters.

## Phase 6: Proof-Time Test (PTT) Review

PTTs exist to exercise Verus-verified iteration and loop forms. If a chapter
has **no iterators and no verified loops** (i.e., no types with `iter()`,
`IntoIterator`, `GhostIterator`, or `ForLoopGhostIterator`, and no `while`/
`for`/`loop` inside `verus!`), then **no PTTs are needed** — note this in
the review and skip the rest of Phase 6.

Otherwise, check that every chapter module has corresponding proof-time tests in
`rust_verify_test/tests/ChapNN/`.

### 6a. Unified test inventory table

Produce a table of source modules against both RTT and PTT files:

| # | Source module | RTT file | PTT file | Status |
|---|-------------|----------|----------|--------|

Status values: Both exist, Missing RTT, Missing PTT, Missing both.

### 6b. Iterator coverage

For each type that implements iteration (has `iter()`, `IntoIterator`,
or a `GhostIterator`/`ForLoopGhostIterator` impl), check that the PTT
exercises iteration. Specifically:

- **loop-match** (`loop { match it.next() { ... } }`) — manual iteration
  with ghost accumulation. The most explicit form; exercises the raw
  iterator protocol.
- **for-iter** (`for x in collection.iter()`) — the standard `for` loop
  over a borrowed iterator.
- **for-consuming** (`for x in collection`) — consuming `IntoIterator`,
  if the type supports it.

Produce a table:

| # | Type | loop-match | for-iter | for-consuming | Notes |
|---|------|-----------|----------|---------------|-------|

Flag types where no iteration PTT exists at all — these are the highest
priority gaps.

### 6c. Loop form coverage

Verus verifies each loop form differently. The PTT should exercise all
loop forms that the source module uses:

- **loop + break** — `loop { ... break; }` with explicit decreases
- **loop-match** — `loop { match it.next() { Some(x) => ..., None => break } }`
- **while** — `while condition { ... }` with invariants and decreases
- **for-range** — `for i in 0..n` with `iter.cur` in invariants
- **for-iter** — `for x in collection.iter()` with iterator ghost state
- **for-consuming** — `for x in collection` consuming the collection

For each source module, list which loop forms appear in the implementation,
then check whether the PTT tests that form. Flag any loop form used in
source but not tested in PTT.

### 6d. Missing PTTs

Propose new PTTs for modules without any. Prioritize:
1. Modules with iterators — iteration is the most fragile proof pattern.
2. Modules with complex loop invariants — the PTT catches invariant regressions.
3. Modules used as building blocks by later chapters.

## Phase 7: Gap Analysis

Two lists:

**Prose items with no implementation:**
- Algorithm X.Y defined in prose but no corresponding function in code
- Theorem stated but not proved (no lemma)
- Cost bound stated but not annotated

**Code with no prose counterpart:**
- Helper functions, Verus scaffolding, overflow lemmas
- These are expected — just note them so the inventory is complete

## Phase 8: Table of Contents Review

Audit each source file against the table-of-contents standard
(`table-of-contents-standard.mdc`). Check:

1. **TOC present** — does the file have a `//  Table of Contents` block?
2. **Section ordering** — are sections in the standard order (1-13)?
3. **In/out placement** — are sections 1-11 inside `verus!` and 12-13 outside?
4. **Section headers** — does each section have its numbered comment header?

Produce the in/out table per the `in-out-table` rule:

| # | File | Clone | PartialEq/Eq | Default | Drop | Iterator | Debug | Display | Macro | Other |
|---|------|:-----:|:------------:|:-------:|:----:|:--------:|:-----:|:-------:|:-----:|-------|

Values: `✅ in`, `✅ out`, `❌ in`, `❌ out`, `-`

Flag any `❌` items or missing TOCs as action items in the review summary.

## Output

- **Cost annotations**: Written directly in source files as doc comments (Phase 3a)
- **Everything else**: Tool output goes to `analyses/` per the classify-spec-strengths rule
- **Review summary**: Write `src/ChapNN/analyses/review-against-prose.md` for each
  reviewed chapter. This is the persistent record of the review. Include:
  - Prose inventory (Phase 2)
  - Cost disagreements found (Phase 3a)
  - Implementation fidelity notes (Phase 3b)
  - Spec fidelity notes (Phase 3c)
  - Parallelism audit table if Mt modules exist (Phase 4)
  - PTT review: unified test table, iterator coverage, loop form coverage (Phase 6)
  - Gap analysis (Phase 7)
  - Table of contents / in-out table (Phase 8)
  - Proof holes summary
  - Date and reviewer

  Use the wide-markdown style header. This file is the single source of truth
  for what was reviewed and what was found — do not rely on chat history.

## Do NOT

- Modify implementation logic, requires/ensures, or function signatures
- Skip the veracity tool and hand-build function inventories
- Write APAS cost lines without reading the prose first
- Write Claude-Opus-4.6 cost lines without reading the implementation
- Leave a function with an APAS cost line but no Claude-Opus-4.6 line (always pair them)
